# -*- coding: utf-8 -*-
"""RF_monitored_features_4_closed_world_optuna.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zy472mt9KQjt2xFYCQMpIK0Lh_zeu4A8
"""

# ============================================
# Closed-world, 0~94 labels, monitored_features_1.csv
# ============================================

import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix
)

import optuna

# -----------------------------
# 0. Seed 고정
# -----------------------------
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# -----------------------------
# 1. 데이터 로드
# -----------------------------
csv_path = "data/preprocessed/monitored_features_4.csv"
df = pd.read_csv(csv_path)

assert "label" in df.columns, "label 컬럼이 없습니다."

feature_cols = [col for col in df.columns if col != "label"]
X = df[feature_cols].values.astype(np.float32)
y = df["label"].values.astype(np.int64)

num_classes = len(np.unique(y))
print("Num samples :", X.shape[0])
print("Num features:", X.shape[1])
print("Num classes :", num_classes)

# -----------------------------
# 2. Train / Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=SEED,
    stratify=y
)

print("Train size:", X_train.shape[0])
print("Test size :", X_test.shape[0])

# -----------------------------
# 3. Optuna Objective 정의
#    - 3-fold StratifiedKFold 사용
#    - macro F1 기준 최대화
# -----------------------------
def objective(trial):
    # 하이퍼파라미터 탐색 범위 정의
    n_estimators = trial.suggest_int("n_estimators", 100, 400)  # 기존 200 포함
    max_depth = trial.suggest_categorical("max_depth", [None, 10, 20, 30, 40, 50])
    min_samples_split = trial.suggest_int("min_samples_split", 2, 10)
    min_samples_leaf = trial.suggest_int("min_samples_leaf", 1, 10)
    max_features = trial.suggest_categorical("max_features", ["sqrt", "log2", 0.3, 0.5, 0.7])

    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        max_features=max_features,
        n_jobs=-1,
        random_state=SEED
    )

    # 3-fold Stratified K-Fold
    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)
    f1_scores = []

    for train_idx, valid_idx in skf.split(X_train, y_train):
        X_tr, X_val = X_train[train_idx], X_train[valid_idx]
        y_tr, y_val = y_train[train_idx], y_train[valid_idx]

        clf.fit(X_tr, y_tr)
        y_pred = clf.predict(X_val)

        f1 = f1_score(y_val, y_pred, average="macro", zero_division=0)
        f1_scores.append(f1)

    # fold 평균 사용
    mean_f1 = float(np.mean(f1_scores))
    return mean_f1

# -----------------------------
# 4. Optuna Study 설정 및 실행
# -----------------------------
study_name = "rf_optuna_tuning"
study = optuna.create_study(
    study_name=study_name,
    direction="maximize"
)

# n_trials = 50
study.optimize(objective, n_trials=50, n_jobs=1)

print("\n=== Optuna Tuning 결과 ===")
print("Best trial:", study.best_trial.number)
print("Best value (macro F1, CV):", study.best_value)
print("Best params:")
for k, v in study.best_params.items():
    print(f"  {k}: {v}")

# -----------------------------
# 5. Best Params로 최종 모델 학습 & Test 평가
# -----------------------------
best_params = study.best_params.copy()

rf_best = RandomForestClassifier(
    n_estimators=best_params["n_estimators"],
    max_depth=best_params["max_depth"],
    min_samples_split=best_params["min_samples_split"],
    min_samples_leaf=best_params["min_samples_leaf"],
    max_features=best_params["max_features"],
    n_jobs=-1,
    random_state=SEED
)

rf_best.fit(X_train, y_train)
y_pred_test = rf_best.predict(X_test)

acc  = accuracy_score(y_test, y_pred_test)
pre  = precision_score(y_test, y_pred_test, average="macro", zero_division=0)
rec  = recall_score(y_test, y_pred_test, average="macro", zero_division=0)
f1   = f1_score(y_test, y_pred_test, average="macro", zero_division=0)

print("\n=== 최종 Test 성능 (Optuna 튜닝 RF) ===")
print(f"Accuracy           : {acc:.4f}")
print(f"Precision (macro)  : {pre:.4f}")
print(f"Recall (macro)     : {rec:.4f}")
print(f"F1-score (macro)   : {f1:.4f}")

# =====================================
# 5) Feature Importance 계산
# =====================================
importances = rf_best.feature_importances_

feature_names = feature_cols  # df에서 label 빼고 뽑았던 컬럼 리스트

fi_df = pd.DataFrame({
    "feature": feature_names,
    "importance": importances
}).sort_values(by="importance", ascending=False)

print("\n=== Top 20 Feature Importances ===")
print(fi_df.head(20))

# -----------------------------
# Feature Importance DataFrame
# -----------------------------
importances = rf_best.feature_importances_
feature_names = feature_cols

fi_df = pd.DataFrame({
    "feature": feature_names,
    "importance": importances
}).sort_values(by="importance", ascending=False)

print("\n=== Top 20 Feature Importances ===")
print(fi_df.head(20))

# -----------------------------
# Feature Importance Plot
# -----------------------------
TOP_N = 15

top_features = fi_df.head(TOP_N)
plt.figure(figsize=(10, 8))
plt.barh(top_features["feature"], top_features["importance"], color="skyblue")
plt.gca().invert_yaxis()  # 가장 중요한 feature가 위로 오도록 뒤집기
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.title(f"Feature Importances (RandomForest + Optuna)")
plt.tight_layout()
plt.show()
